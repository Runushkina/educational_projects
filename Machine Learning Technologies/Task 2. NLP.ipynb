{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2"
      ],
      "metadata": {
        "id": "4v7qdjqXu-tG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\tDownload Alice in Wonderland by Lewis Carroll from Project Gutenberg's website http://www.gutenberg.org/files/11/11-0.txt"
      ],
      "metadata": {
        "id": "Fl6K-XBRwn92"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEiRyqz_u4J_",
        "outputId": "62e71b7d-2db7-4e6b-b788-21d635c989ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "import requests\n"
      ],
      "metadata": {
        "id": "kNWWSrkMvBSW"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "file_path = '/content/drive/MyDrive/Alice.txt'\n",
        "with open(file_path, 'r') as file:\n",
        "    text = file.read()"
      ],
      "metadata": {
        "id": "v483xdQgvbjC"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chapter_names = text.split('CHAPTER')[1:13]"
      ],
      "metadata": {
        "id": "GU0lh8_ymn5y"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.split('CHAPTER')[13:]"
      ],
      "metadata": {
        "id": "C8Kh9ZSxmoBj"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Perform any necessary preprocessing on the text:\n",
        "- including converting to lower case\n",
        "- removing stop words, numbers, non-alphabetic characters\n",
        "- lemmatization"
      ],
      "metadata": {
        "id": "6QKE5vj1-agY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Natural Language for Processing"
      ],
      "metadata": {
        "id": "CGctV6SsL_HA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "T7GTXfUhLrQm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c89c3ff7-36d8-46b9-fd80-f9c3cd765918"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to lowercase\n",
        "text = [text_part.lower() for text_part in text]"
      ],
      "metadata": {
        "id": "FPpWFxPae7DD"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove non-alphabetic characters and numbers\n",
        "text = [re.sub(r'[^a-z ]', '', text_part) for text_part in text]"
      ],
      "metadata": {
        "id": "VaN4YuvZe7Jy"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text\n",
        "text = [word_tokenize(text_part) for text_part in text]"
      ],
      "metadata": {
        "id": "cmyAAftpe7Ta"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "text = [[token for token in text_part if token not in stop_words] for text_part in text]"
      ],
      "metadata": {
        "id": "kSyIiHgBgE3f"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "text = [[lemmatizer.lemmatize(token) for token in text_part] for text_part in text]"
      ],
      "metadata": {
        "id": "XqEUuMfjkkvV"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.\tFind Top 10 most important (for example, in terms of TF-IDF metric) words from each chapter in the text (not \"Alice\")\n",
        "\n",
        "- How would you name each chapter according to the identified tokens?"
      ],
      "metadata": {
        "id": "SzD9SheylCYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "rfbqk7S0nVeP"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find top 10 words for each chapter\n",
        "top_words_by_chapter = []\n",
        "for chapter in text:\n",
        "    # Calculate TF-IDF scores for words in the chapter\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform([token for token in chapter if token != \"alice\"])\n",
        "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "    top_words = [feature_names[i] for i in tfidf_matrix.sum(axis=0).argsort()[0, -10:][::-1]]\n",
        "    top_words_by_chapter.append(top_words)"
      ],
      "metadata": {
        "id": "QlhPgjRHpcwt"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, top_words in enumerate(top_words_by_chapter):\n",
        "    chapter_name = ', '.join(top_words[0][0])\n",
        "    print(f\"Chapter {i+1}: {chapter_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTQEL5X7pc0y",
        "outputId": "cb67ec1e-b7f8-4387-f385-008661243df5"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter 1: thought, could, door, nothing, think, one, way, see, like, little\n",
            "Chapter 2: oh, went, cried, im, dear, foot, thing, said, mouse, little\n",
            "Chapter 3: ill, must, one, thing, long, soon, know, dodo, mouse, said\n",
            "Chapter 4: voice, quite, bill, thought, heard, get, one, rabbit, said, little\n",
            "Chapter 5: time, youth, ive, size, serpent, pigeon, im, little, caterpillar, said\n",
            "Chapter 6: could, went, baby, little, footman, much, duchess, like, cat, said\n",
            "Chapter 7: one, went, thing, know, time, hare, march, dormouse, hatter, said\n",
            "Chapter 8: began, went, three, two, see, cat, king, head, queen, said\n",
            "Chapter 9: moral, say, dont, queen, went, gryphon, duchess, turtle, mock, said\n",
            "Chapter 10: could, join, wont, lobster, beautiful, would, gryphon, turtle, mock, said\n",
            "Chapter 11: court, thought, rabbit, witness, queen, one, dormouse, hatter, king, said\n",
            "Chapter 12: king, copy, term, gutenberg, electronic, foundation, gutenbergtm, said, work, project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chapter 1 -> Even one little thought could help find the way\n",
        "\n",
        "Chapter 2 -> Oh, dear little mouse\n",
        "\n",
        "Chapter 3 -> I must know about one long think\n",
        "\n",
        "Chapter 4 -> A little rabbit heard one quite voice\n",
        "\n",
        "Chapter 5 -> I was a little caterpillar in youth\n",
        "\n",
        "Chapter 6 -> One little baby likes a duchess\n",
        "\n",
        "Chapter 7 -> One thing went wrong - time\n",
        "\n",
        "Chapter 8 -> Three Queens and two Kings see cat\n",
        "\n",
        "Chapter 9 -> Moral says don't mock at Queen\n",
        "\n",
        "Chapter 10 -> A beautiful lobster joins a turtle\n",
        "\n",
        "Chapter 11 -> Hatter brought a witness to the king's court\n",
        "\n",
        "Chapter 12 -> King terms electronic project foundation"
      ],
      "metadata": {
        "id": "UCkTIwxp2WsD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.\tFind the Top 10 most used verbs in sentences with Alice.\n",
        "- What does Alice do most often?"
      ],
      "metadata": {
        "id": "mvGqO1UQ6jq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4vaG2Os6Xo2",
        "outputId": "1e5b411b-2402-4bdb-ff1b-3fc1926a1842"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "file_path = '/content/drive/MyDrive/Alice.txt'\n",
        "with open(file_path, 'r') as file:\n",
        "    text = file.read()"
      ],
      "metadata": {
        "id": "in8aNy1k94oS"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize sentences\n",
        "sentences = re.split(r'[.!?]', text)"
      ],
      "metadata": {
        "id": "udz1KUOg6XtA"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to lowercase\n",
        "sentences = [sentence.lower() for sentence in sentences]"
      ],
      "metadata": {
        "id": "8y0tYFjJnVj7"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove non-alphabetic characters and numbers\n",
        "sentences = [re.sub(r'[^a-z ]', '', sentence) for sentence in sentences]"
      ],
      "metadata": {
        "id": "SszLK_lvnVpu"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text\n",
        "sentences = [word_tokenize(sentence) for sentence in sentences]"
      ],
      "metadata": {
        "id": "m2yDr2cZ_iA7"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove stop words\n",
        "sentences = [[word for word in sentence if word not in stop_words] for sentence in sentences]"
      ],
      "metadata": {
        "id": "d9HiUpkQ_ogt"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatization\n",
        "sentences = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in sentences]"
      ],
      "metadata": {
        "id": "SWnl2vBL_ol4"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find verb in sentences with Alica\n",
        "alice_verbs = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    if 'alice' in sentence:\n",
        "        words = word_tokenize(' '.join(sentence))\n",
        "        tagged_words = pos_tag(words)\n",
        "        # Extract verbs\n",
        "        verbs = [word for word, pos in tagged_words if pos.startswith('VB')]\n",
        "        alice_verbs.extend(verbs)\n",
        "\n",
        "# Count verb occurrences\n",
        "verb_counts = Counter(alice_verbs)"
      ],
      "metadata": {
        "id": "HEMWyQUV_op0"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top 10 most used verbs:\")\n",
        "\n",
        "for verb in verb_counts.most_common(10):\n",
        "    print(f\"{verb[0]} = {verb[1]}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeqdKzjn_ouQ",
        "outputId": "39622a3f-e8ea-4930-9251-9f41387e8185"
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 most used verbs:\n",
            "said = 156\n",
            "thought = 33\n",
            "went = 23\n",
            "looked = 18\n",
            "say = 17\n",
            "see = 15\n",
            "got = 15\n",
            "know = 15\n",
            "think = 14\n",
            "began = 14\n"
          ]
        }
      ]
    }
  ]
}